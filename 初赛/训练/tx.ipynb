{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ä¸‹è½½éœ€è¦çš„åº“"
      ],
      "metadata": {
        "id": "q900OSDDYOuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "5H6FhZNwV_le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cac11b-34bc-49b9-95cb-fa3d43b65e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "å¯¼å…¥éœ€è¦çš„åº“"
      ],
      "metadata": {
        "id": "M0EShsJEYTJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "GKdJB1PfWFHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å®šä¹‰æ¨¡å‹å‚æ•°"
      ],
      "metadata": {
        "id": "iTeyKWX1YWS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-5\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the language model\n",
        "#MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" # use this to finetune the sentiment classifier\n"
      ],
      "metadata": {
        "id": "-SqZKVRwYVma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ·»åŠ æ•°æ®é›†"
      ],
      "metadata": {
        "id": "DggqunYMYgtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡\n",
        "#è®­ç»ƒé›†\n",
        "myDataset={}\n",
        "myDataset[\"train\"]={}\n",
        "myDataset[\"env\"]={}\n",
        "myDataset[\"train\"][\"data\"]=[]\n",
        "myDataset[\"train\"][\"label\"]=[]\n",
        "myDataset[\"env\"][\"data\"]=[]\n",
        "myDataset[\"env\"][\"label\"]=[]\n",
        "with open(\"/content/drive/MyDrive/data/train.txt\",\"r\",encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        label,data=line.split(\"|\")\n",
        "        myDataset[\"train\"][\"data\"].append(data)\n",
        "        myDataset[\"train\"][\"label\"].append(int(label))\n",
        "with open(\"/content/drive/MyDrive/data/output.txt\",\"r\",encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "      data,label=line.split(\"||\")\n",
        "      myDataset[\"train\"][\"data\"].append(data)\n",
        "      myDataset[\"train\"][\"label\"].append(int(label))\n",
        "#éªŒè¯é›†\n",
        "with open(\"/content/drive/MyDrive/data/dev_en.txt\",\"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        label,data=line.split(\"|\")\n",
        "        myDataset[\"env\"][\"data\"].append(data)\n",
        "        myDataset[\"env\"][\"label\"].append(int(label))\n",
        "with open(\"/content/drive/MyDrive/data/dev_pt.txt\",\"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        label,data=line.split(\"|\")\n",
        "        myDataset[\"env\"][\"data\"].append(data)\n",
        "        myDataset[\"env\"][\"label\"].append(int(label))\n",
        "myDataset[\"train\"][\"data\"].extend(myDataset[\"env\"][\"data\"])\n",
        "myDataset[\"train\"][\"label\"].extend(myDataset[\"env\"][\"label\"])"
      ],
      "metadata": {
        "id": "1kTfKWT9YiNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(myDataset[\"train\"][\"label\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlBp5P7V2XJc",
        "outputId": "2303cc41-706c-4c61-fdfd-e41a2a8624d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
      ],
      "metadata": {
        "id": "vor7hIoZYp2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51367218-dfa7-4988-e40e-5d991a13afaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(myDataset['train']['data'], truncation=True, padding=True)\n",
        "val_encodings = tokenizer(myDataset['env']['data'], truncation=True, padding=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgk-h5q4YxcJ",
        "outputId": "1e5c7ba0-8279-4bbe-b55a-83f6b209b82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ„å»ºæ•°æ®é›†\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "    def __getitem__(self,idx):\n",
        "        item={key :torch.tensor(value[idx]) for key,value in self.data.items()}\n",
        "        item[\"label\"]=self.label[idx]\n",
        "        return item\n",
        "train_dataset = MyDataset(train_encodings, myDataset['train']['label'])\n",
        "val_dataset = MyDataset(val_encodings, myDataset['env']['label'])\n",
        "for val in val_dataset:\n",
        "  print(val)"
      ],
      "metadata": {
        "id": "LcRjq6s3ZXAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxwirdJnOzuv"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate --upgrade\n",
        "!pip install transformers[torch]\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=\"./result\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',                     # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/bm\", num_labels=2)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                              # the instantiated ğŸ¤— Transformers model to be trained\n",
        "    args=training_args,                       # training arguments, defined above\n",
        "    train_dataset=train_dataset,              # training dataset\n",
        "    eval_dataset=val_dataset                  # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0OHP_02gaOuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "trainer.save_model(\"./results/best_model\") # save best model\n",
        "model.save_pretrained('./bm')"
      ],
      "metadata": {
        "id": "rvcUo0pQaQax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_raw, test_labels , _ = trainer.predict(val_dataset)\n",
        "test_preds = np.argmax(test_preds_raw, axis=-1)\n",
        "print(classification_report(test_labels, test_preds, digits=3))"
      ],
      "metadata": {
        "id": "uLR20Ra5aRlS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f0e68587-87f7-4784-fd9f-a43f2a0fa940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.909     0.966     0.936       145\n",
            "           1      0.891     0.745     0.812        55\n",
            "\n",
            "    accuracy                          0.905       200\n",
            "   macro avg      0.900     0.855     0.874       200\n",
            "weighted avg      0.904     0.905     0.902       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# å‡è®¾test_predsæ˜¯æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œtrue_labelsæ˜¯çœŸå®çš„æ ‡ç­¾\n",
        "report = classification_report(test_labels, test_preds, digits=3)\n",
        "\n",
        "# è§£æåˆ†ç±»æŠ¥å‘Šå¹¶è·å–æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡å’Œå¬å›ç‡\n",
        "lines = report.split('\\n')\n",
        "precision = []\n",
        "recall = []\n",
        "for line in lines[2:4]:\n",
        "    row = line.split()\n",
        "    precision.append(float(row[1]))\n",
        "    recall.append(float(row[2]))\n",
        "\n",
        "# è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡å’Œå¬å›ç‡\n",
        "for i in range(2):\n",
        "    print(f\"Precision: {precision[i]}\")\n",
        "    print(f\"Recall: {recall[i]}\")\n",
        "res=((1+0.49)*precision[0]*recall[0]/(0.49*precision[0]+recall[0])+(1+0.49)*precision[1]*recall[1]/(0.49*precision[1]+recall[1]))/2\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLnr-sO_fhzZ",
        "outputId": "5d0a5971-18b4-4785-d220-8fc4263da75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.909\n",
            "Recall: 0.966\n",
            "Precision: 0.891\n",
            "Recall: 0.745\n",
            "0.8820209377785959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r bm /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "OWBrcAf0uPD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r tokenizer /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "wGa3ed_Pi6lB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}